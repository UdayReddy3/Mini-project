"""
Optimized Plant Disease Detection Model (Fast Training)
This module contains an optimized CNN model using MobileNetV2 for faster training
while maintaining high accuracy (~90-92%).
Optimizations:
- MobileNetV2 instead of ResNet50 (2-3x faster)
- 192x192 image size instead of 224x224 (40% faster)
- 20 epochs max with early stopping (stops when accuracy plateaus)
- Batch size 64 for faster processing
- Aggressive data augmentation for better generalization
"""

import os
import numpy as np
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers, models
from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from sklearn.metrics import classification_report, accuracy_score
import matplotlib.pyplot as plt
import json


class OptimizedPlantDiseaseModel:
    """
    Optimized model class for faster training using MobileNetV2.
    """
    
    def __init__(self, img_size=192, num_classes=38):
        """
        Initialize the optimized model parameters.
        
        Args:
            img_size (int): Target image size for MobileNetV2 (default: 192 - faster than 224)
            num_classes (int): Number of disease classes (PlantVillage has ~38 classes)
        """
        self.img_size = img_size
        self.num_classes = num_classes
        self.model = None
        self.class_names = None
        
    def load_plantvillage_dataset(self, data_path, validation_split=0.2):
        """
        Load the PlantVillage dataset with optimized settings for faster training.
        
        Args:
            data_path (str): Path to the PlantVillage dataset
            validation_split (float): Fraction of data for validation
            
        Returns:
            tuple: (train_dataset, val_dataset)
        """
        print(f"Loading PlantVillage dataset from {data_path}...")
        print("Using optimized settings for fast training...")
        
        # Create image data generators with aggressive augmentation
        train_datagen = ImageDataGenerator(
            rescale=1.0/255.0,
            rotation_range=35,              # Reduced from 40
            width_shift_range=0.15,         # Reduced from 0.2
            height_shift_range=0.15,        # Reduced from 0.2
            shear_range=0.15,               # Reduced from 0.2
            zoom_range=0.15,                # Reduced from 0.2
            horizontal_flip=True,
            fill_mode='nearest',
            validation_split=validation_split
        )
        
        val_datagen = ImageDataGenerator(rescale=1.0/255.0)
        
        # Load training data with LARGER batch size for speed
        train_dataset = train_datagen.flow_from_directory(
            data_path,
            target_size=(self.img_size, self.img_size),
            batch_size=64,  # Larger batch for faster processing
            class_mode='categorical',
            subset='training',
            shuffle=True
        )
        
        # Load validation data
        val_dataset = train_datagen.flow_from_directory(
            data_path,
            target_size=(self.img_size, self.img_size),
            batch_size=64,
            class_mode='categorical',
            subset='validation',
            shuffle=False
        )
        
        # Store class names
        self.class_names = list(train_dataset.class_indices.keys())
        self.num_classes = len(self.class_names)
        
        print(f"\n‚úì Found {self.num_classes} disease classes")
        print(f"‚úì Training samples: {train_dataset.samples}")
        print(f"‚úì Validation samples: {val_dataset.samples}")
        print(f"‚úì Image size: {self.img_size}x{self.img_size}")
        print(f"‚úì Batch size: 64 (optimized for speed)")
        
        return train_dataset, val_dataset
    
    def build_model(self, freeze_base=True, num_frozen_layers=120):
        """
        Build the model using transfer learning with MobileNetV2 (MUCH FASTER).
        
        Args:
            freeze_base (bool): Whether to freeze base model layers
            num_frozen_layers (int): Number of layers to freeze
            
        Returns:
            model: Compiled Keras model
        """
        print("\n" + "="*60)
        print("Building OPTIMIZED transfer learning model with MobileNetV2...")
        print("="*60)
        
        # Load pre-trained MobileNetV2 (MUCH LIGHTER than ResNet50)
        base_model = MobileNetV2(
            weights='imagenet',
            include_top=False,
            input_shape=(self.img_size, self.img_size, 3)
        )
        
        # Freeze base model layers
        if freeze_base:
            for layer in base_model.layers[:num_frozen_layers]:
                layer.trainable = False
            print(f"‚úì Froze first {num_frozen_layers} layers of MobileNetV2")
        
        # Build custom top layers (simplified for speed)
        model = models.Sequential([
            base_model,
            layers.GlobalAveragePooling2D(),
            layers.Dense(128, activation='relu', kernel_regularizer=keras.regularizers.l2(0.001)),
            layers.BatchNormalization(),
            layers.Dropout(0.4),
            layers.Dense(self.num_classes, activation='softmax')
        ])
        
        self.model = model
        return model
    
    def compile_model(self, learning_rate=5e-4):
        """
        Compile the model with optimizer, loss, and metrics.
        
        Args:
            learning_rate (float): Learning rate for Adam optimizer
        """
        optimizer = keras.optimizers.Adam(learning_rate=learning_rate)
        
        self.model.compile(
            optimizer=optimizer,
            loss='categorical_crossentropy',
            metrics=['accuracy']
        )
        
        print("\n‚úì Model compiled successfully")
        print(self.model.summary())
    
    def train(self, train_dataset, val_dataset, epochs=20, model_save_path='models/plant_disease_model.h5'):
        """
        Train the model with aggressive early stopping for fast convergence.
        
        Args:
            train_dataset: Training dataset
            val_dataset: Validation dataset
            epochs (int): Maximum epochs (default: 20 - early stopping will likely stop earlier)
            model_save_path (str): Path to save the best model
            
        Returns:
            history: Training history object
        """
        print("\n" + "="*60)
        print("Starting FAST model training...")
        print("="*60)
        
        # Create callbacks for aggressive early stopping
        callbacks = [
            EarlyStopping(
                monitor='val_loss',
                patience=5,              # Stop after 5 epochs of no improvement (aggressive)
                restore_best_weights=True,
                verbose=1,
                min_delta=0.001         # Minimum change to qualify as improvement
            ),
            ModelCheckpoint(
                model_save_path,
                monitor='val_accuracy',
                save_best_only=True,
                verbose=1
            ),
            ReduceLROnPlateau(
                monitor='val_loss',
                factor=0.5,
                patience=3,              # Reduce LR more aggressively
                min_lr=1e-7,
                verbose=1
            )
        ]
        
        print(f"\nTraining will run for maximum {epochs} epochs")
        print("(Early stopping may stop earlier if accuracy plateaus)\n")
        
        # Train the model
        history = self.model.fit(
            train_dataset,
            validation_data=val_dataset,
            epochs=epochs,
            callbacks=callbacks,
            verbose=1
        )
        
        return history
    
    def evaluate(self, val_dataset):
        """
        Evaluate the model on validation data.
        
        Args:
            val_dataset: Validation dataset
            
        Returns:
            dict: Dictionary containing accuracy, loss, and metrics
        """
        print("\n" + "="*60)
        print("Evaluating model on validation set...")
        print("="*60)
        
        loss, accuracy = self.model.evaluate(val_dataset, verbose=1)
        print(f"\n‚úì Validation Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")
        print(f"‚úì Validation Loss: {loss:.4f}")
        
        # Get predictions and true labels
        y_true = []
        y_pred = []
        
        for images, labels in val_dataset:
            predictions = self.model.predict(images, verbose=0)
            # `labels` may be a Tensor (tf.data) or a NumPy array (ImageDataGenerator).
            # Handle both cases robustly.
            if hasattr(labels, 'numpy'):
                lbls = labels.numpy()
            else:
                lbls = labels
                # labels may be a NumPy array (from ImageDataGenerator) or a TF tensor
                # using np.argmax works for both types
                y_true.extend(np.argmax(labels, axis=1))
            y_pred.extend(np.argmax(predictions, axis=1))
        
        # Generate classification report
        report = classification_report(
            y_true, y_pred,
            target_names=self.class_names,
            output_dict=True
        )
        
        print("\n" + "="*60)
        print("Classification Report:")
        print("="*60)
        print(classification_report(
            y_true, y_pred,
            target_names=self.class_names
        ))
        
        metrics = {
            'accuracy': float(accuracy),
            'loss': float(loss),
            'classification_report': report
        }
        
        return metrics
    
    def plot_training_history(self, history, save_path='models/training_history.png'):
        """
        Plot training and validation accuracy/loss curves.
        
        Args:
            history: Training history object
            save_path (str): Path to save the plot
        """
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        # Plot accuracy
        axes[0].plot(history.history['accuracy'], label='Train Accuracy', linewidth=2)
        axes[0].plot(history.history['val_accuracy'], label='Val Accuracy', linewidth=2)
        axes[0].set_title('Model Accuracy', fontsize=12, fontweight='bold')
        axes[0].set_xlabel('Epoch')
        axes[0].set_ylabel('Accuracy')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        # Plot loss
        axes[1].plot(history.history['loss'], label='Train Loss', linewidth=2)
        axes[1].plot(history.history['val_loss'], label='Val Loss', linewidth=2)
        axes[1].set_title('Model Loss', fontsize=12, fontweight='bold')
        axes[1].set_xlabel('Epoch')
        axes[1].set_ylabel('Loss')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"\n‚úì Training history plot saved to {save_path}")
        plt.close()
    
    def save_model(self, model_path='models/plant_disease_model.h5'):
        """Save the trained model."""
        self.model.save(model_path)
        print(f"‚úì Model saved to {model_path}")
    
    def save_class_names(self, class_names_path='models/class_names.json'):
        """Save class names for inference."""
        with open(class_names_path, 'w') as f:
            json.dump(self.class_names, f, indent=2)
        print(f"‚úì Class names saved to {class_names_path}")


def main():
    """
    Main function to run the optimized training pipeline.
    Total estimated time: 2-3 hours (vs 25-30 hours for ResNet50)
    """
    print("\n" + "="*60)
    print("üöÄ FAST PLANT DISEASE DETECTION MODEL TRAINING")
    print("="*60)
    print("Optimizations:")
    print("  ‚Ä¢ MobileNetV2 (2-3x faster than ResNet50)")
    print("  ‚Ä¢ 192x192 images (40% faster than 224x224)")
    print("  ‚Ä¢ Max 20 epochs with aggressive early stopping")
    print("  ‚Ä¢ Batch size 64 for parallel processing")
    print("  ‚Ä¢ Expected accuracy: 90-92%")
    print("  ‚Ä¢ Estimated time: 2-3 hours")
    print("="*60 + "\n")
    
    # Initialize optimized model
    model_handler = OptimizedPlantDiseaseModel(img_size=192, num_classes=38)
    
    # Load dataset
    data_path = 'data/PlantVillage/raw/color'
    
    try:
        train_dataset, val_dataset = model_handler.load_plantvillage_dataset(
            data_path,
            validation_split=0.2
        )
    except Exception as e:
        print(f"\n‚ùå Error loading dataset: {e}")
        print("Please ensure the PlantVillage dataset is in the correct location:")
        print(f"  {os.path.abspath(data_path)}")
        return
    
    # Build model
    model_handler.build_model(freeze_base=True, num_frozen_layers=120)
    model_handler.compile_model(learning_rate=5e-4)
    
    # Train model
    history = model_handler.train(
        train_dataset,
        val_dataset,
        epochs=20,  # Max 20 epochs, but early stopping will likely stop earlier
        model_save_path='models/best_model.h5'
    )
    
    # Plot training history
    model_handler.plot_training_history(history)
    
    # Evaluate model
    metrics = model_handler.evaluate(val_dataset)
    
    # Save model and class names
    model_handler.save_model('models/plant_disease_model.h5')
    model_handler.save_class_names('models/class_names.json')
    
    # Save metrics
    with open('models/metrics.json', 'w') as f:
        json.dump({
            'accuracy': metrics['accuracy'],
            'loss': metrics['loss'],
            'classification_report': metrics['classification_report'],
            'model_type': 'MobileNetV2',
            'image_size': 192,
            'training_mode': 'Optimized (Fast)'
        }, f, indent=4)
    
    print("\n" + "="*60)
    print("‚úÖ TRAINING COMPLETED SUCCESSFULLY!")
    print("="*60)
    print(f"Model Accuracy: {metrics['accuracy']:.4f} ({metrics['accuracy']*100:.2f}%)")
    print(f"Model Loss: {metrics['loss']:.4f}")
    print("\nFiles saved:")
    print("  ‚úì models/plant_disease_model.h5")
    print("  ‚úì models/class_names.json")
    print("  ‚úì models/metrics.json")
    print("  ‚úì models/training_history.png")
    print("\nYou can now run the web app:")
    print("  streamlit run app.py")
    print("="*60 + "\n")


if __name__ == '__main__':
    main()
